"""
CLIPå›¾åƒåˆ†æå·¥å…·
ä¸ºAgentæ·»åŠ è§†è§‰ç†è§£èƒ½åŠ›
"""

import os
from typing import List, Dict, Optional
from PIL import Image


class CLIPTools:
    """CLIPå›¾åƒåˆ†æå·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–CLIPæ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        self.model = None
        self.preprocess = None
        self.device = None
        self._initialized = False
    
    def _initialize(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼‰"""
        if self._initialized:
            return True
        
        try:
            import torch
            import clip
            
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½CLIPæ¨¡å‹ (è®¾å¤‡: {self.device})...")
            
            self.model, self.preprocess = clip.load("ViT-L/14@336px", device=self.device)
            self._initialized = True
            
            print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except ImportError:
            print("âŒ CLIPåº“æœªå®‰è£…")
            print("ğŸ’¡ å®‰è£…æ–¹æ³•: pip install git+https://github.com/openai/CLIP.git")
            return False
        except Exception as e:
            print(f"âŒ CLIPåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def classify_image(self, image_and_labels: str) -> str:
        """
        å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»
        
        å‚æ•°æ ¼å¼: "å›¾ç‰‡è·¯å¾„|||æ ‡ç­¾1,æ ‡ç­¾2,æ ‡ç­¾3"
        ä¾‹å¦‚: "photo.jpg|||çŒ«,ç‹—,é¸Ÿ,é±¼"
        
        è¿”å›: æ¯ä¸ªæ ‡ç­¾çš„æ¦‚ç‡
        """
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            if not self._initialize():
                return "é”™è¯¯: CLIPæ¨¡å‹æœªèƒ½åˆå§‹åŒ–ï¼Œè¯·å…ˆå®‰è£…CLIPåº“"
            
            # è§£æå‚æ•°
            parts = image_and_labels.split("|||")
            if len(parts) != 2:
                return "é”™è¯¯: å‚æ•°æ ¼å¼åº”ä¸º 'å›¾ç‰‡è·¯å¾„|||æ ‡ç­¾1,æ ‡ç­¾2,æ ‡ç­¾3'"
            
            image_path = parts[0].strip()
            labels = [label.strip() for label in parts[1].split(",")]
            
            # æ£€æŸ¥æ–‡ä»¶
            if not os.path.exists(image_path):
                return f"é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ {image_path} ä¸å­˜åœ¨"
            
            # åŠ è½½å’Œå¤„ç†å›¾åƒ
            import torch
            import clip
            
            image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)
            text = clip.tokenize(labels).to(self.device)
            
            # æ¨ç†
            with torch.no_grad():
                logits_per_image, logits_per_text = self.model(image, text)
                probs = logits_per_image.softmax(dim=-1)
            
            # æ ¼å¼åŒ–ç»“æœ
            result = f"å›¾åƒåˆ†ç±»ç»“æœ ({image_path}):\n\n"
            
            # æŒ‰æ¦‚ç‡æ’åº
            sorted_indices = probs[0].argsort(descending=True)
            
            for idx in sorted_indices:
                label = labels[idx]
                prob = probs[0][idx].item()
                bar = "â–ˆ" * int(prob * 50)
                result += f"{label:20s} {prob:6.2%} {bar}\n"
            
            return result
            
        except Exception as e:
            return f"å›¾åƒåˆ†ç±»é”™è¯¯: {str(e)}"
    
    def search_images(self, query_and_folder: str) -> str:
        """
        åœ¨æ–‡ä»¶å¤¹ä¸­æœç´¢æœ€åŒ¹é…çš„å›¾ç‰‡
        
        å‚æ•°æ ¼å¼: "æœç´¢æ–‡æœ¬|||å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„"
        ä¾‹å¦‚: "å¤•é˜³ä¸‹çš„æµ·æ»©|||./photos"
        
        è¿”å›: æŒ‰ç›¸ä¼¼åº¦æ’åºçš„å›¾ç‰‡åˆ—è¡¨
        """
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            if not self._initialize():
                return "é”™è¯¯: CLIPæ¨¡å‹æœªèƒ½åˆå§‹åŒ–"
            
            # è§£æå‚æ•°
            parts = query_and_folder.split("|||")
            if len(parts) != 2:
                return "é”™è¯¯: å‚æ•°æ ¼å¼åº”ä¸º 'æœç´¢æ–‡æœ¬|||å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„'"
            
            query_text = parts[0].strip()
            folder_path = parts[1].strip()
            
            # æ£€æŸ¥æ–‡ä»¶å¤¹
            if not os.path.exists(folder_path):
                return f"é”™è¯¯: æ–‡ä»¶å¤¹ {folder_path} ä¸å­˜åœ¨"
            
            # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp')
            image_files = []
            
            for filename in os.listdir(folder_path):
                if filename.lower().endswith(image_extensions):
                    image_files.append(filename)
            
            if not image_files:
                return f"é”™è¯¯: æ–‡ä»¶å¤¹ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶"
            
            # åŠ è½½æ‰€æœ‰å›¾ç‰‡
            import torch
            import clip
            
            images = []
            valid_files = []
            
            for img_file in image_files:
                try:
                    img_path = os.path.join(folder_path, img_file)
                    img = self.preprocess(Image.open(img_path)).unsqueeze(0)
                    images.append(img)
                    valid_files.append(img_file)
                except Exception as e:
                    print(f"è·³è¿‡ {img_file}: {e}")
            
            if not images:
                return "é”™è¯¯: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾ç‰‡"
            
            images = torch.cat(images).to(self.device)
            text = clip.tokenize([query_text]).to(self.device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            with torch.no_grad():
                image_features = self.model.encode_image(images)
                text_features = self.model.encode_text(text)
                
                # å½’ä¸€åŒ–
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)
                
                # ç›¸ä¼¼åº¦
                similarity = (image_features @ text_features.T).squeeze(-1)
            
            # ç¡®ä¿similarityæ˜¯1ç»´çš„
            if similarity.dim() == 0:
                similarity = similarity.unsqueeze(0)
            
            # æ’åºç»“æœ
            sorted_indices = similarity.argsort(descending=True)
            
            result = f"æœç´¢ç»“æœ (æŸ¥è¯¢: '{query_text}'):\n\n"
            result += f"åœ¨ {folder_path} ä¸­æ‰¾åˆ° {len(valid_files)} å¼ å›¾ç‰‡\n\n"
            
            # æ˜¾ç¤ºå‰10ä¸ªç»“æœ
            for i, idx in enumerate(sorted_indices[:10], 1):
                filename = valid_files[idx]
                score = similarity[idx].item()
                bar = "â–ˆ" * int(score * 50)
                result += f"{i:2d}. {filename:30s} {score:6.3f} {bar}\n"
            
            return result
            
        except Exception as e:
            return f"å›¾åƒæœç´¢é”™è¯¯: {str(e)}"
    
    def understand_image(self, image_path: str) -> str:
        """
        ç†è§£å›¾åƒå†…å®¹ï¼ˆåˆ†æå¤šä¸ªç»´åº¦ï¼‰
        
        è¾“å…¥: å›¾ç‰‡è·¯å¾„
        
        è¿”å›: åœºæ™¯ã€æ—¶é—´ã€å¤©æ°”ã€æƒ…ç»ªç­‰å¤šç»´åº¦åˆ†æ
        """
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            if not self._initialize():
                return "é”™è¯¯: CLIPæ¨¡å‹æœªèƒ½åˆå§‹åŒ–"
            
            # æ£€æŸ¥æ–‡ä»¶
            if not os.path.exists(image_path):
                return f"é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ {image_path} ä¸å­˜åœ¨"
            
            # å®šä¹‰å¤šç»´åº¦é—®é¢˜
            questions = {
                "åœºæ™¯ç±»å‹": ["å®¤å†…åœºæ™¯", "æˆ·å¤–åœºæ™¯", "åŸå¸‚è¡—é“", "è‡ªç„¶é£æ™¯", "å»ºç­‘ç‰©"],
                "æ—¶é—´": ["ç™½å¤©", "å¤œæ™š", "é»„æ˜", "æ¸…æ™¨", "ä¸­åˆ"],
                "å¤©æ°”": ["æ™´å¤©", "é˜´å¤©", "é›¨å¤©", "é›ªå¤©", "å¤šäº‘"],
                "äººç‰©": ["æœ‰äººç‰©", "æ— äººç‰©", "å•äºº", "å¤šäºº", "äººç¾¤"],
                "æƒ…ç»ªæ°›å›´": ["æ¬¢å¿«çš„", "å¹³é™çš„", "å¿§éƒçš„", "æ¿€åŠ¨çš„", "ç¥ç§˜çš„"]
            }
            
            import torch
            import clip
            
            # åŠ è½½å›¾åƒ
            image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)
            
            result = f"å›¾åƒå†…å®¹åˆ†æ ({image_path}):\n\n"
            
            # é€ä¸ªç»´åº¦åˆ†æ
            for category, options in questions.items():
                text = clip.tokenize(options).to(self.device)
                
                with torch.no_grad():
                    logits_per_image, logits_per_text = self.model(image, text)
                    probs = logits_per_image.softmax(dim=-1)
                
                # æ‰¾å‡ºæœ€å¯èƒ½çš„é€‰é¡¹
                best_idx = probs[0].argmax().item()
                best_option = options[best_idx]
                best_prob = probs[0][best_idx].item()
                
                result += f"ã€{category}ã€‘: {best_option} (ç½®ä¿¡åº¦: {best_prob:.1%})\n"
                
                # æ˜¾ç¤ºå…¶ä»–é€‰é¡¹
                result += "  å…¶ä»–å¯èƒ½: "
                other_probs = []
                for i, option in enumerate(options):
                    if i != best_idx:
                        other_probs.append(f"{option}({probs[0][i].item():.0%})")
                result += ", ".join(other_probs[:3]) + "\n\n"
            
            return result
            
        except Exception as e:
            return f"å›¾åƒç†è§£é”™è¯¯: {str(e)}"
    
    def compare_images(self, images_str: str) -> str:
        """
        æ¯”è¾ƒå¤šå¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦
        
        å‚æ•°æ ¼å¼: "å›¾ç‰‡1è·¯å¾„,å›¾ç‰‡2è·¯å¾„,å›¾ç‰‡3è·¯å¾„,..."
        ä¾‹å¦‚: "cat1.jpg,cat2.jpg,dog.jpg"
        
        è¿”å›: å›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            if not self._initialize():
                return "é”™è¯¯: CLIPæ¨¡å‹æœªèƒ½åˆå§‹åŒ–"
            
            # è§£æå‚æ•°
            image_paths = [path.strip() for path in images_str.split(",")]
            
            if len(image_paths) < 2:
                return "é”™è¯¯: è‡³å°‘éœ€è¦2å¼ å›¾ç‰‡è¿›è¡Œæ¯”è¾ƒ"
            
            # æ£€æŸ¥æ–‡ä»¶
            for path in image_paths:
                if not os.path.exists(path):
                    return f"é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ {path} ä¸å­˜åœ¨"
            
            # åŠ è½½æ‰€æœ‰å›¾ç‰‡
            import torch
            
            images = []
            for img_path in image_paths:
                img = self.preprocess(Image.open(img_path)).unsqueeze(0)
                images.append(img)
            
            images = torch.cat(images).to(self.device)
            
            # è®¡ç®—ç‰¹å¾
            with torch.no_grad():
                features = self.model.encode_image(images)
                features /= features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
                similarity_matrix = features @ features.T
            
            # æ ¼å¼åŒ–ç»“æœ
            result = "å›¾ç‰‡ç›¸ä¼¼åº¦åˆ†æ:\n\n"
            
            # æ˜¾ç¤ºæ–‡ä»¶åï¼ˆç®€åŒ–ï¼‰
            short_names = [os.path.basename(path)[:20] for path in image_paths]
            
            # è¡¨å¤´
            result += "      "
            for name in short_names:
                result += f"{name:22s}"
            result += "\n" + "-" * (6 + 22 * len(short_names)) + "\n"
            
            # ç›¸ä¼¼åº¦çŸ©é˜µ
            for i, name1 in enumerate(short_names):
                result += f"{name1:20s}  "
                for j in range(len(short_names)):
                    sim = similarity_matrix[i][j].item()
                    result += f"{sim:5.2f}  "
                    result += "â–ˆ" * int(sim * 10) + "  "
                result += "\n"
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å›¾ç‰‡å¯¹
            result += "\næœ€ç›¸ä¼¼çš„å›¾ç‰‡å¯¹:\n"
            max_sim = 0
            max_pair = (0, 0)
            
            for i in range(len(image_paths)):
                for j in range(i + 1, len(image_paths)):
                    sim = similarity_matrix[i][j].item()
                    if sim > max_sim:
                        max_sim = sim
                        max_pair = (i, j)
            
            result += f"  {short_names[max_pair[0]]} â†” {short_names[max_pair[1]]}\n"
            result += f"  ç›¸ä¼¼åº¦: {max_sim:.2%}\n"
            
            return result
            
        except Exception as e:
            return f"å›¾ç‰‡æ¯”è¾ƒé”™è¯¯: {str(e)}"


# åˆ›å»ºå…¨å±€å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_clip_tools = CLIPTools()


def create_clip_tools():
    """åˆ›å»ºCLIPå·¥å…·åˆ—è¡¨ï¼ˆç”¨äºé›†æˆåˆ°Agentï¼‰"""
    from langchain_core.tools import Tool
    
    tools = [
        Tool(
            name="classify_image",
            func=_clip_tools.classify_image,
            description="ä½¿ç”¨CLIPå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è¾“å…¥æ ¼å¼ï¼š'å›¾ç‰‡è·¯å¾„|||æ ‡ç­¾1,æ ‡ç­¾2,æ ‡ç­¾3'ã€‚Classify image with CLIP."
        ),
        Tool(
            name="search_images",
            func=_clip_tools.search_images,
            description="åœ¨æ–‡ä»¶å¤¹ä¸­æœç´¢æœ€åŒ¹é…çš„å›¾ç‰‡ã€‚è¾“å…¥æ ¼å¼ï¼š'æœç´¢æ–‡æœ¬|||æ–‡ä»¶å¤¹è·¯å¾„'ã€‚Search images in folder."
        ),
        Tool(
            name="understand_image",
            func=_clip_tools.understand_image,
            description="ç†è§£å›¾åƒå†…å®¹ï¼Œåˆ†æåœºæ™¯ã€æ—¶é—´ã€å¤©æ°”ç­‰ã€‚è¾“å…¥ï¼šå›¾ç‰‡è·¯å¾„ã€‚Understand image content."
        ),
        Tool(
            name="compare_images",
            func=_clip_tools.compare_images,
            description="æ¯”è¾ƒå¤šå¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦ã€‚è¾“å…¥æ ¼å¼ï¼š'å›¾ç‰‡1,å›¾ç‰‡2,å›¾ç‰‡3,...'ã€‚Compare image similarity."
        ),
    ]
    
    return tools


if __name__ == "__main__":
    """æµ‹è¯•CLIPå·¥å…·"""
    print("=" * 60)
    print("CLIPå·¥å…·æµ‹è¯•")
    print("=" * 60)
    
    clip_tools = CLIPTools()
    
    print("\n1. æµ‹è¯•æ¨¡å‹åŠ è½½:")
    if clip_tools._initialize():
        print("âœ… CLIPå·¥å…·å¯ç”¨")
    else:
        print("âŒ CLIPå·¥å…·ä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…CLIPåº“")
        print("   å®‰è£…å‘½ä»¤: pip install git+https://github.com/openai/CLIP.git")

